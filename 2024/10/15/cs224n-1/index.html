<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="天泽龟的龟壳屋">
  <link 
    rel="icon" 
    href="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
  <title>CS224N 学习随笔</title><meta name="robots" content="noindex">
  
    
      <meta 
        property="og:title" 
        content="CS224N 学习随笔">
    
    
      <meta 
        property="og:url" 
        content="https://tzturtle.moe/2024/10/15/cs224n-1/index.html">
    
    
      <meta 
        property="og:img" 
        content="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
    
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2024-10-15">
      <meta 
        property="og:article:modified_time" 
        content="2024-11-08">
      <meta 
        property="og:article:author" 
        content="天泽龟">
      
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.4.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img" 
          src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">天泽龟的龟壳屋</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/bangumis" 
        class="navbar-menu-item">
        
          番剧
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      CS224N 学习随笔
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2024-10-15T01:51:12.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2024-10-15</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/" 
          class="post-meta-link">
          专业学习
        </a>
      
    
    
      <span class="dot"></span>
      <span>3.9k 字</span>
    
  </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <ul>
<li>我们有很多参数的模型，正则化！ 一个完整的损失函数包括所有参数的正则化，例如 L2 正则化。
<ul>
<li><strong>机器学习时代</strong>：当我们有很多特征（或后来一个非常强大/深入的模型等）时，正则化可以防止过拟合；</li>
<li><strong>大模型时代</strong>：当我们有一个大模型时，正则化产生的模型可以很好地泛化；
<ul>
<li>我们不在乎我们的模型在训练数据上过拟合， 即使他们非常过分，因为：</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image-20241023112402862.png" alt="image-20241023112402862" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/image-20241023112402862.png" class="lozad post-image"></p>
<br>
<ul>
<li><strong>权重初始化</strong>：通常在神经网络中，我们会将权重初始化为较小的随机值，而不是零矩阵。原因是，使用零值初始化会导致对称性问题；</li>
<li><strong>优化器（Optimizers）</strong>：<code>Adagrad</code> 是最简单的自适应优化器之一，它会根据各个参数的历史梯度来调整每个参数的学习率；<code>Adam</code> 是一种常用且表现较好的优化器，它结合了 RMSprop 和动量（momentum）更新的优点；</li>
</ul>
<p>第九页的内容提到了“语言建模”（Language Modeling），是自然语言处理中的一个核心任务。这一页的重点是：</p>
<ol>
<li><strong>任务定义</strong>：语言建模的任务是预测下一个词。比如句子“The students opened their ______”，一个语言模型的目标就是去预测填入这个空格的词语。可能的答案可以是“books”“laptops”等等。</li>
<li><strong>形式化定义</strong>：给定一个单词序列 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(w_1, w_2, \dots, w_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，语言模型会计算下一个词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的概率分布 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_t|w_1, w_2, \dots, w_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。简单来说，这个模型要基于上下文来预测接下来的单词的可能性。</li>
<li><strong>模型的应用</strong>：这样的模型可以理解为给一段文字分配一个概率，这样便能根据其可能性评估该段文字。例如，句子 “The cat sat on the mat” 有一个概率 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>The cat sat on the mat</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{The cat sat on the mat})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">The cat sat on the mat</span></span><span class="mclose">)</span></span></span></span>，用来表征该段文本出现的自然程度。</li>
</ol>
<ul>
<li>
<p>过去的语言模型都用的是 “n-gram 词袋模型”，假设当前单词仅由前 n 个词决定，感觉有点落后了。</p>
<ul>
<li><strong>上下文限制</strong>：n-gram 模型仅考虑固定数量的前 n 个词，这使得它无法捕捉到更长的上下文信息。许多语言现象需要更广泛的上下文才能正确理解，比如歧义、长距离依赖关系等。</li>
<li><strong>稀疏性问题</strong>：随着 n 的增加，可能的 n-gram 组合数量会迅速增长，导致模型在训练数据中遇到许多未见过的组合（即稀疏性问题）。这使得模型难以泛化到新数据上。</li>
<li><strong>缺乏语义理解</strong>：n-gram 模型通常依赖于表面形式的统计特征，而忽略了词汇之间的语义关系。它们无法捕捉同义词、反义词等语义信息。</li>
</ul>
</li>
<li>
<p>RNN 呢？序列模型，梯度消失，很难学会在许多时间步长中保留信息。</p>
</li>
<li>
<p>LSTM 缓解了问题，但序列模型就是慢。</p>
</li>
<li>
<p><strong>我们为什么应该关心语言建模？</strong></p>
<ul>
<li>语言建模是一项基准任务（Benchmark），可帮助我们衡量预测语言使用的进展 ；</li>
<li>语言建模是许多 NLP 任务的子任务，尤其是那些涉及生成文本或估计文本概率的任务；</li>
</ul>
</li>
</ul>
<br>
<h2 id="lecture-8-transformer-basis"><a class="markdownIt-Anchor" href="#lecture-8-transformer-basis"></a> Lecture 8: Transformer Basis</h2>
<h3 id="the-building-block-we-need-self-attention-to-generate-y-we-need-to-pay-attention-to-y_le-t"><a class="markdownIt-Anchor" href="#the-building-block-we-need-self-attention-to-generate-y-we-need-to-pay-attention-to-y_le-t"></a> The building block we need: <strong>self attention</strong> —— to generate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝑦</mi></mrow><annotation encoding="application/x-tex">𝑦</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>, we need to pay attention to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mrow><mo>≤</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{\le t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.675739em;vertical-align:-0.24517899999999998em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.295179em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24517899999999998em;"><span></span></span></span></span></span></span></span></span></span>.</h3>
<p>Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1:n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> be a sequence of words in vocabulary <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝑉</mi></mrow><annotation encoding="application/x-tex">𝑉</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>, like Zuko made his uncle tea.</p>
<p>For each <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒘</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">𝒘_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.11111em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.11111em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><mi>𝐸</mi><msub><mi>𝒘</mi><mi>𝒊</mi></msub></mrow><annotation encoding="application/x-tex">𝒙_i = 𝐸𝒘_𝒊</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.12583em;">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.12583em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.11111em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.11111em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.11387em;">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> , where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝐸</mi><mo>∈</mo><msup><mi mathvariant="normal">R</mi><mrow><mi>d</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow></msup></mrow><annotation encoding="application/x-tex">𝐸 ∈ ℝ^{d×|V|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord amsrm">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">×</span><span class="mord mtight">∣</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span>  is an embedding matrix.</p>
<ol>
<li>
<p>Transform each word embedding with weight matrices Q,K,V , each in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="normal">R</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">ℝ^{d\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord amsrm">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝒒</mi><mo>=</mo><mi>𝑄</mi><msub><mi>𝒙</mi><mi>𝒊</mi></msub></mrow><annotation encoding="application/x-tex">𝒒=𝑄𝒙_𝒊</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="mord boldsymbol" style="margin-right:0.105em;">q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.12583em;">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.12583em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.11387em;">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (queries) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝒌</mi><mo>=</mo><mi>𝐾</mi><msub><mi>𝒙</mi><mi>𝒊</mi></msub></mrow><annotation encoding="application/x-tex">𝒌=𝐾𝒙_𝒊</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord boldsymbol" style="margin-right:0.11111em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.12583em;">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.12583em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.11387em;">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (keys) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝒗</mi><mo>=</mo><mi>𝑉</mi><msub><mi>𝒙</mi><mi>𝒊</mi></msub></mrow><annotation encoding="application/x-tex">𝒗=𝑉𝒙_𝒊</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord boldsymbol" style="margin-right:0.11111em;">v</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.12583em;">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.12583em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.11387em;">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (values)</p>
</li>
<li>
<p>Compute pairwise similarities between keys and queries; normalize with <code>softmax</code>.</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝒆</mi><mo>=</mo><msubsup><mi>𝒒</mi><mi>𝒊</mi><mi>T</mi></msubsup><msub><mi>𝒌</mi><mi>𝒋</mi></msub></mrow><annotation encoding="application/x-tex">𝒆 =𝒒_𝒊^T 𝒌_𝒋</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord boldsymbol" style="margin-right:0.085em;">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1274389999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.105em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4177180000000003em;margin-left:-0.105em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.11387em;">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2822819999999999em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.11111em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3352819999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.11111em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.1672em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> ， <span class='katex-error' title='Error: Font metrics not found for font: .'>𝜶_{i,j} = \frac{exp(𝒆_{i,j})}{∑_j exp(𝒆_{i,j})}</span> ；</p>
</li>
<li>
<p>Compute output for each word as weighted sum of values: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mo>∑</mo><msub><mi>α</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_i = \sum\alpha_{i,j} v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
</li>
</ol>
<br>
<h3 id="barriers-and-solutions-for-self-attention-as-a-building-block"><a class="markdownIt-Anchor" href="#barriers-and-solutions-for-self-attention-as-a-building-block"></a> Barriers and solutions for Self-Attention as a building block</h3>
<p><img src="image-20241107161059107.png" alt="image-20241107161059107" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20241107161059107.png" class="lozad post-image"></p>
<br>
<h3 id="multi-head-self-attention-is-computationally-efficient"><a class="markdownIt-Anchor" href="#multi-head-self-attention-is-computationally-efficient"></a> Multi-head self-attention is computationally efficient</h3>
<p>Now that we’ve replaced self attention with multi-head self attention, we’ll go through two  optimization tricks that end up  being:  1. <strong>Residual Connections</strong>; 2. <strong>Layer Normalization</strong>. In most Transformer diagrams,  these are written together as “Add &amp; Norm”.</p>
<p>Layer normalization is a trick to help models train faster. Core Idea is to cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation <strong>within each layer</strong>.</p>
<br>
<h3 id="the-transformer-encoder-decoder"><a class="markdownIt-Anchor" href="#the-transformer-encoder-decoder"></a> The Transformer Encoder-Decoder</h3>
<p>Recall that in machine  translation, we processed the  source sentence with a  bidirectional model and  generated the target with a unidirectional model.</p>
<p>In the decoder, we have attention that looks more like what we saw.</p>
<ul>
<li>Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>h</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">ℎ_1 ,…,ℎ_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> be output vectors from the Transformer encoder;  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑥</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="normal">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">𝑥_i ∈ ℝ^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord amsrm">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></li>
<li>Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑧</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>𝑧</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">𝑧_1 ,…,𝑧_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> be input vectors from the Transformer decoder, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑧</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="normal">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">𝑧_i ∈ ℝ^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord amsrm">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></li>
<li>Then keys and values are drawn from the encoder (like a memory): • <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑘</mi><mi>i</mi></msub><mo>=</mo><mi>𝐾</mi><msub><mi>h</mi><mi>i</mi></msub><mtext> </mtext><mo separator="true">,</mo><msub><mi>𝑣</mi><mi>i</mi></msub><mo>=</mo><mi>𝑉</mi><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">𝑘_i=𝐾ℎ_i ~, 𝑣_i=𝑉ℎ_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace nobreak"> </span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> .</li>
<li>And the queries are drawn from the  decoder, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑞</mi><mi>i</mi></msub><mo>=</mo><mi>𝑄</mi><msub><mi>𝑧</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">𝑞_i = 𝑄𝑧_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
</ul>
<br>
<h3 id="what-would-we-like-to-fix-about-the-transformer"><a class="markdownIt-Anchor" href="#what-would-we-like-to-fix-about-the-transformer"></a> What would we like to fix about the Transformer?</h3>
<p><strong>Quadratic compute in self-attention</strong>!!!</p>
<ul>
<li>Computing all pairs of interactions means our computation grows quadratically with the sequence length! For recurrent models, it only grew linearly!</li>
<li>However, as Transformers grow larger, a larger and larger percent of compute is outside the self-attention portion, despite the quadratic cost. In practice, almost no large Transformer language models use anything but the quadratic cost attention we’ve presented here.</li>
</ul>
<br>
<h2 id="lecture-9-pretraining"><a class="markdownIt-Anchor" href="#lecture-9-pretraining"></a> Lecture 9: Pretraining</h2>
<p>预训练革命是指预训练模型在自然语言处理领域带来的重大变革。预训练模型直接用互联网上的各种信息进行训练，得到一个模型基座（Foundation Model），再通过微调的方式将该模型适应到各种子任务上。这样做的好处是：</p>
<ol>
<li><strong>处理大规模、多样化的数据集</strong>：预训练模型能够处理互联网上的大规模、多样化的数据集。</li>
<li><strong>无需标记数据</strong>：由于预训练不依赖于标记数据，这使得模型能够扩展到更大的规模。</li>
<li><strong>计算效率</strong>：预训练模型需要在计算上进行优化，以适应大规模数据的处理，Transformer 支持并行化。</li>
</ol>
<br>
<h3 id="the-byte-pair-encoding-algorithm"><a class="markdownIt-Anchor" href="#the-byte-pair-encoding-algorithm"></a> The byte-pair encoding algorithm</h3>
<p>这是一种在自然语言处理（NLP）中用于处理词汇以下级别的结构的方法，也就是我们所说的子词建模。</p>
<p>BPE 是一种简单而有效的子词词汇构建策略，它的基本步骤如下：</p>
<ol>
<li><strong>初始化词汇表</strong>：开始时，词汇表只包含字符和一个表示单词结束的符号（如<code>&lt;/w&gt;</code>）。</li>
<li><strong>寻找最常见的字符对</strong>：在给定的文本语料库中，找出最常见的相邻字符对，例如“th”或“he”。</li>
<li><strong>添加为子词并替换</strong>：将这个字符对添加到词汇表中，并在文本中将这对字符替换为一个新的子词。例如，如果“th”是最常见的字符对，我们就将其添加为一个子词，并在所有出现“th”的地方替换它。</li>
<li><strong>重复直至达到期望的词汇表大小</strong>：重复上述过程，直到词汇表达到我们设定的大小。</li>
</ol>
<p>然而，这种编码方式并没有兼顾到单词的上下文。“一个词的完整意义总是依赖于上下文的，任何脱离完整上下文的意义研究都不能被认真对待。” 使用预训练的词嵌入（如 word2vec 或 GloVe）捕捉了词的分布特性，但不包含上下文信息。这意味着每个词都被赋予了一个固定向量，无论它出现在哪个句子或上下文中，这个向量都是相同的。</p>
<p>这种方法面临的挑战包括：</p>
<ol>
<li><strong>训练数据的局限性</strong>：为了教会模型语言的所有上下文方面，我们需要大量的下游任务训练数据（例如问答任务）。</li>
<li><strong>随机初始化的参数</strong>：网络中的大多数参数都是随机初始化的，这意味着模型需要从零开始学习语言的复杂性。</li>
</ol>
<p>假设我们有一个句子：“电影很好看。” 如果我们使用没有预训练的模型，电影这个词的嵌入是固定的，不随上下文变化。但是，如果我们使用预训练的模型，它需要能够根据整个句子的上下文来调整“电影”这个词的表示（哪个电影？）。</p>
<p>但在现代NLP中，几乎所有的参数都是通过预训练来初始化的。这意味着，我们不再从随机初始化开始训练模型，而是使用预训练模型作为起点，这有助于我们更快地收敛到一个好的解决方案，并且能够更好地捕捉语言的复杂性。</p>
<br>
<h3 id="pretraining-for-three-types-of-architectures"><a class="markdownIt-Anchor" href="#pretraining-for-three-types-of-architectures"></a> Pretraining for three types of architectures</h3>
<p>编码器通常用于理解文本，并且能够捕捉到双向的上下文信息，我们可以用填空的方式去构建预训练模型的任务。提出了一个创新的想法：Masked Language Model（MLM）。这种方法的核心思想是：在输入序列中，随机选择一定比例的词，并用一个特殊的[MASK]标记替换这些词。然后，训练模型来预测这些被掩盖的词。</p>
<p>Devlin et al., 2018 proposed the “Masked LM” objective and released the weights of a  pretrained Transformer, a model they labeled BERT.  Some more details about Masked LM for BERT:</p>
<ul>
<li>Predict a random 15% of (sub)word tokens.</li>
<li>Replace input word with [MASK] 80% of the time.</li>
<li>Replace input word with a random token 10% of the time.</li>
<li>Leave input word unchanged 10% of the time (but  still predict it!)</li>
</ul>
<p>Pretraining is expensive and impractical on a single GPU, BERT was pretrained with 64 TPU chips for a total of 4 days (TPUs are special tensor operation acceleration hardware). Finetuning is practical and common on a single GPU: <strong>“Pretrain once, finetune many times.”</strong></p>
<br>
<h3 id="limitations-of-pretrained-encoders"><a class="markdownIt-Anchor" href="#limitations-of-pretrained-encoders"></a> Limitations of pretrained encoders</h3>
<p>尽管预训练编码器（encoders）在许多NLP任务上表现出色，但它们并非万能的，出于以下原因限制：</p>
<ol>
<li><strong>自回归生成的限制</strong>：
<ul>
<li>预训练编码器，如BERT，通常不直接用于生成序列，因为它们不是为自回归（autoregressive）生成设计的。自回归生成意味着模型在生成序列时是逐词进行的，每个词的生成依赖于之前生成的词。</li>
</ul>
</li>
<li><strong>缺乏未来上下文</strong>：
<ul>
<li>编码器在处理输入时只能看到过去的上下文，而不能利用未来的词。这对于生成任务来说是一个限制，因为在生成新词时，Encoder 模型需要考虑到未来的上下文信息。</li>
</ul>
</li>
</ol>
<p>Extensions of BERT A takeaway from the <code>RoBERTa</code> paper: more compute, more data can improve pretraining  even when not changing the underlying Transformer encoder.</p>
<br>
<h3 id="pretraining-encoder-decoders-what-pretraining-objective-to-use"><a class="markdownIt-Anchor" href="#pretraining-encoder-decoders-what-pretraining-objective-to-use"></a> Pretraining encoder-decoders: what pretraining objective to use?</h3>
<p>编码器-解码器模型是一种常用于序列到序列任务（如机器翻译）的神经网络架构。这种模型包含两个部分：编码器和解码器。编码器处理输入序列并生成上下文表示，而解码器则基于这个表示训练整个模型。</p>
<p>编码器-解码器模型是一种常用于序列到序列任务（如机器翻译）的神经网络架构。这种模型包含两个部分：编码器和解码器。编码器处理输入序列并生成上下文表示，而解码器则基于这个表示来生成输出序列。</p>
<p>预训练过程可以描述为以下步骤：</p>
<ol>
<li><strong>编码器处理输入</strong>：编码器处理整个输入序列 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mo>∗</mo><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_1,...,*w_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6597200000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>*，并生成一系列隐藏状态 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_1,...,h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li><strong>解码器预测输出</strong>：解码器接收编码器的输出，并尝试基于部分输入和之前预测的词，预测输出序列 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">w_{t+1}...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span></li>
</ol>
<p>T5（Text-to-Text Transfer Transformer）是 Raffel 等人提出的一个预训练模型，他将 Span Corruption 作为训练目标。当一部分输入被 masked 时，模型必须依赖于剩余的上下文来预测缺失的部分，这促使模型捕捉到更丰富的语言特征。<strong>T5模型的一个优秀的特性：</strong> 它能够在广泛的开放领域问题上微调，并且能很好地检索其参数知识。</p>
<br>
<h3 id="pretraining-decoders"><a class="markdownIt-Anchor" href="#pretraining-decoders"></a> Pretraining decoders</h3>
<ul>
<li>Language models! What we’ve seen so far.</li>
<li>Nice to generate from; can’t condition on future words.</li>
<li>All the biggest pretrained models are Decoders.</li>
</ul>
<p><strong>Decoder 的任务本质上就是在最后一个单词的 hidden state 上做分类任务！</strong> 只不过作为一个语言模型，这个分类任务是在词表上做的，最终 Decoder 可以作为一个 Generator 学会如何根据给定的上下文预测下一个词的概率分布，生成连贯且语法正确的文本序列。</p>
<p>代表作：Generative Pretrained Transformer (GPT)，GPT模型是第一个大规模预训练的Transformer解码器，使用了12层的Transformer Decoder结构，拥有1.17亿个参数。</p>
<br>
<h3 id="gpt-3-in-context-learning-and-very-large-models"><a class="markdownIt-Anchor" href="#gpt-3-in-context-learning-and-very-large-models"></a> GPT-3, In-context learning, and very large models</h3>
<p>GPT-3 是 <code>OpenAI</code> 开发的一种非常大的语言模型，它不仅在规模上超越了以往的模型，而且在能力上也展现出了新的特性，特别是上下文学习的能力——这意味着，通过在输入中提供一些示例，GPT-3 能够理解并执行特定的任务。</p>
<p>这揭示了预训练模型的两种互动方式：</p>
<ul>
<li><strong>采样</strong>：我们可以从 GPT-3 定义的概率分布中采样，提供提示（prompt）来生成文本。</li>
<li><strong>微调</strong>：我们可以微调 GPT-3 来关注特定的任务，并获取其预测。</li>
</ul>
<br>
<h3 id="why-scale-scaling-laws"><a class="markdownIt-Anchor" href="#why-scale-scaling-laws"></a> Why scale? Scaling laws</h3>
<p>在自然语言处理和机器学习领域，模型规模的扩大通常会导致模型性能的提升。以下是扩大模型规模的一些原因：</p>
<ol>
<li><strong>更强的表示能力</strong>：更大的模型拥有更多的参数，这使得它们能够捕捉和学习更复杂的模式和表示。</li>
<li><strong>更好的泛化能力</strong>：大规模模型往往能够更好地泛化到未见过的数据上，因为它们拥有更丰富的知识库。</li>
<li><strong>更高的数据效率</strong>：大规模模型通常能够更有效地利用数据，因为它们能够从大量的参数中学习到更一般化的特征。</li>
</ol>
<p>Scaling Laws 随着模型规模和数据规模的增加，模型的性能通常会提高 — — 这通常被称作是 Pre-Scaling Laws.</p>
<h3 id="scaling-efficiency-how-do-we-best-use-our-compute"><a class="markdownIt-Anchor" href="#scaling-efficiency-how-do-we-best-use-our-compute"></a> Scaling Efficiency: how do we best use our compute</h3>
<p>尽管GPT-3的规模很大，但这并不意味着它是最优的参数和数据量组合。在有限的计算资源下，需要在模型的参数数量和训练数据量之间找到平衡。有时候，一个较小的模型在较少的数据上训练可能比一个较大的模型在更多的数据上训练更有效。这是因为较小的模型可能更容易训练，并且能够更快地收敛。</p>
<br>
<h3 id="security-and-membership-inference"><a class="markdownIt-Anchor" href="#security-and-membership-inference"></a> Security and Membership Inference</h3>
<p>\Membership Inference Attack 展示了两个模型：一个目标模型（Target Model）和一个攻击模型（Attack Model）。</p>
<ol>
<li><strong>目标模型（Target Model）</strong>：
<ul>
<li>这个模型是攻击者想要推断其训练数据的模型。它接收输入数据（data record），并输出对应的类别标签（class label）。目标模型的预测结果（prediction）是基于其训练数据集学习到的模式。</li>
</ul>
</li>
<li><strong>攻击模型（Attack Model）</strong>：
<ul>
<li>攻击模型的目的是预测给定的数据记录（data）是否属于目标模型的训练集。</li>
<li>攻击模型输出一个标签（label），这个标签指示输入数据是否是目标模型训练集的一部分；</li>
</ul>
</li>
<li><strong>预测流程</strong>：
<ul>
<li>当攻击者拥有一个数据点时，他们首先使用目标模型进行预测。</li>
<li>然后，攻击者使用攻击模型来预测这个数据点是否是目标模型训练集的成员。</li>
</ul>
</li>
</ol>
<!-- flag of hidden posts -->
  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            天泽龟
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://tzturtle.moe/2024/10/15/cs224n-1/">
            https://tzturtle.moe/2024/10/15/cs224n-1/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: '4CiiPBXpbjDnPvIIwfuEPEY6-gzGzoHsz',
        appKey: '3AQY35K3Laq9fLvTG2uOHDUT',
        placeholder: '留下你的评论...',
        path: window.location.pathname,
        avatar: 'identicon',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'true',
        requiredFields: [],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-8-transformer-basis"><span class="toc-text"> Lecture 8: Transformer Basis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-building-block-we-need-self-attention-to-generate-y-we-need-to-pay-attention-to-y_le-t"><span class="toc-text"> The building block we need: self attention —— to generate 𝑦𝑦y, we need to pay attention to y≤ty_{\le t}y≤t​.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#barriers-and-solutions-for-self-attention-as-a-building-block"><span class="toc-text"> Barriers and solutions for Self-Attention as a building block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-attention-is-computationally-efficient"><span class="toc-text"> Multi-head self-attention is computationally efficient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-transformer-encoder-decoder"><span class="toc-text"> The Transformer Encoder-Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-would-we-like-to-fix-about-the-transformer"><span class="toc-text"> What would we like to fix about the Transformer?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-9-pretraining"><span class="toc-text"> Lecture 9: Pretraining</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-byte-pair-encoding-algorithm"><span class="toc-text"> The byte-pair encoding algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-for-three-types-of-architectures"><span class="toc-text"> Pretraining for three types of architectures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-pretrained-encoders"><span class="toc-text"> Limitations of pretrained encoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-encoder-decoders-what-pretraining-objective-to-use"><span class="toc-text"> Pretraining encoder-decoders: what pretraining objective to use?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-decoders"><span class="toc-text"> Pretraining decoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-3-in-context-learning-and-very-large-models"><span class="toc-text"> GPT-3, In-context learning, and very large models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why-scale-scaling-laws"><span class="toc-text"> Why scale? Scaling laws</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scaling-efficiency-how-do-we-best-use-our-compute"><span class="toc-text"> Scaling Efficiency: how do we best use our compute</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#security-and-membership-inference"><span class="toc-text"> Security and Membership Inference</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
    class="author-img" 
    alt="author avatar">

<p class="author-name">天泽龟</p>
<p class="author-description">天泽龟的龟是龟裂的龟哦。</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>49</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>13</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/12645985">
          <i class="iconfont icon-bilibili society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/TURLEing">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://turleing.github.io/about/">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-8-transformer-basis"><span class="toc-text"> Lecture 8: Transformer Basis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-building-block-we-need-self-attention-to-generate-y-we-need-to-pay-attention-to-y_le-t"><span class="toc-text"> The building block we need: self attention —— to generate 𝑦𝑦y, we need to pay attention to y≤ty_{\le t}y≤t​.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#barriers-and-solutions-for-self-attention-as-a-building-block"><span class="toc-text"> Barriers and solutions for Self-Attention as a building block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-attention-is-computationally-efficient"><span class="toc-text"> Multi-head self-attention is computationally efficient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-transformer-encoder-decoder"><span class="toc-text"> The Transformer Encoder-Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-would-we-like-to-fix-about-the-transformer"><span class="toc-text"> What would we like to fix about the Transformer?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-9-pretraining"><span class="toc-text"> Lecture 9: Pretraining</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-byte-pair-encoding-algorithm"><span class="toc-text"> The byte-pair encoding algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-for-three-types-of-architectures"><span class="toc-text"> Pretraining for three types of architectures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-pretrained-encoders"><span class="toc-text"> Limitations of pretrained encoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-encoder-decoders-what-pretraining-objective-to-use"><span class="toc-text"> Pretraining encoder-decoders: what pretraining objective to use?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-decoders"><span class="toc-text"> Pretraining decoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-3-in-context-learning-and-very-large-models"><span class="toc-text"> GPT-3, In-context learning, and very large models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why-scale-scaling-laws"><span class="toc-text"> Why scale? Scaling laws</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scaling-efficiency-how-do-we-best-use-our-compute"><span class="toc-text"> Scaling Efficiency: how do we best use our compute</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#security-and-membership-inference"><span class="toc-text"> Security and Membership Inference</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E7%94%9F%E6%B4%BB%E5%88%86%E4%BA%AB/">
        <div class="categories-list-item">
          生活分享
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/">
        <div class="categories-list-item">
          算法竞赛
          <span class="categories-list-item-badge">12</span>
        </div>
      </a>
    
      <a href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          专业学习
          <span class="categories-list-item-badge">21</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          科研学习
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A4%BE%E5%9B%A2%E6%8E%A8%E9%80%81/">
        <div class="categories-list-item">
          社团推送
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" 
        title="计算机组成原理">
        <div class="tags-list-item">计算机组成原理</div>
      </a>
    
      <a 
        href="/tags/%E9%9A%8F%E7%AC%94/" 
        title="随笔">
        <div class="tags-list-item">随笔</div>
      </a>
    
      <a 
        href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" 
        title="操作系统">
        <div class="tags-list-item">操作系统</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" 
        title="强化学习">
        <div class="tags-list-item">强化学习</div>
      </a>
    
      <a 
        href="/tags/NLP/" 
        title="NLP">
        <div class="tags-list-item">NLP</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" 
        title="字符串">
        <div class="tags-list-item">字符串</div>
      </a>
    
      <a 
        href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" 
        title="分布式">
        <div class="tags-list-item">分布式</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" 
        title="大模型">
        <div class="tags-list-item">大模型</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F/" 
        title="多项式">
        <div class="tags-list-item">多项式</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" 
        title="强连通分量">
        <div class="tags-list-item">强连通分量</div>
      </a>
    
      <a 
        href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" 
        title="推荐系统">
        <div class="tags-list-item">推荐系统</div>
      </a>
    
      <a 
        href="/tags/%E7%BA%BF%E6%80%A7%E5%9F%BA/" 
        title="线性基">
        <div class="tags-list-item">线性基</div>
      </a>
    
      <a 
        href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" 
        title="博弈论">
        <div class="tags-list-item">博弈论</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-8-transformer-basis"><span class="toc-text"> Lecture 8: Transformer Basis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-building-block-we-need-self-attention-to-generate-y-we-need-to-pay-attention-to-y_le-t"><span class="toc-text"> The building block we need: self attention —— to generate 𝑦𝑦y, we need to pay attention to y≤ty_{\le t}y≤t​.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#barriers-and-solutions-for-self-attention-as-a-building-block"><span class="toc-text"> Barriers and solutions for Self-Attention as a building block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-attention-is-computationally-efficient"><span class="toc-text"> Multi-head self-attention is computationally efficient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-transformer-encoder-decoder"><span class="toc-text"> The Transformer Encoder-Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-would-we-like-to-fix-about-the-transformer"><span class="toc-text"> What would we like to fix about the Transformer?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-9-pretraining"><span class="toc-text"> Lecture 9: Pretraining</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-byte-pair-encoding-algorithm"><span class="toc-text"> The byte-pair encoding algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-for-three-types-of-architectures"><span class="toc-text"> Pretraining for three types of architectures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-pretrained-encoders"><span class="toc-text"> Limitations of pretrained encoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-encoder-decoders-what-pretraining-objective-to-use"><span class="toc-text"> Pretraining encoder-decoders: what pretraining objective to use?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretraining-decoders"><span class="toc-text"> Pretraining decoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-3-in-context-learning-and-very-large-models"><span class="toc-text"> GPT-3, In-context learning, and very large models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why-scale-scaling-laws"><span class="toc-text"> Why scale? Scaling laws</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scaling-efficiency-how-do-we-best-use-our-compute"><span class="toc-text"> Scaling Efficiency: how do we best use our compute</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#security-and-membership-inference"><span class="toc-text"> Security and Membership Inference</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-10-29</div>
        <a href="/2024/10/29/rl-2/"><div class="recent-posts-item-content">《动手学强化学习》学习笔记【二】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-10-28</div>
        <a href="/2024/10/28/parallel-training-survey/"><div class="recent-posts-item-content">【转载】浅谈大模型分布式训练并行技术</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-10-22</div>
        <a href="/2024/10/22/rl-1/"><div class="recent-posts-item-content">《动手学强化学习》学习笔记【一】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-10-15</div>
        <a href="/2024/10/15/openai-o1-survey/"><div class="recent-posts-item-content">OpenAI o1 调查报告</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2024
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          天泽龟的龟壳屋
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton" 
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
  </body>
</html>
